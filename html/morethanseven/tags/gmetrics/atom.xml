<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: gmetrics | Morethanseven]]></title>
  <link href="http://www.morethanseven.net/tags/gmetrics/atom.xml" rel="self"/>
  <link href="http://www.morethanseven.net/"/>
  <updated>2014-01-02T18:08:37+00:00</updated>
  <id>http://www.morethanseven.net/</id>
  <author>
    <name><![CDATA[Gareth Rushgrove]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Very Simple Custom Ganglia Metrics]]></title>
    <link href="http://www.morethanseven.net/2010/06/01/Very-simple-custom-ganglia-metrics/"/>
    <updated>2010-06-01T00:00:00+01:00</updated>
    <id>http://www.morethanseven.net/2010/06/01/Very-simple-custom-ganglia-metrics</id>
    <content type="html"><![CDATA[<p>Logging useful information from running systems for monitoring purposes is pretty important if you want to see how your software is behaving in the real world. It's one thing to test something locally, another to test something under load on a testing environment and quite something else to watch production code while running.</p>

<p>The numbers can be useful for checking newly released code isn't having a detrimental effect on performance, observing what changes in load are doing to systems over time and planning for future capacity growth.</p>

<p>Creating log files, agregating files from multiple machines and then analysing the results is one approach. Another is using something like "Ganglia":http://ganglia.sourceforge.net/. Ganglia is great for trending data over time, and ties in nicely to Nagios for reporting. Installing the monitoring daemon on machines and generally getting the default checks (memory, disk, network, etc.) up and running is nice and easy. From there using the gmetric command line to create custom metrics (say checking some mysql statistics) is again straight forward.</p>

<p>So far, so good. The only issue I've run into was creating custom metrics on the fly from a machine outside the network. For bonus points these metrics were nothing to do with the machine on which they were collected, but to do with the system overall. More specifically the metrics were web site performance data gathered via some cucumber and "celerity":http://celerity.rubyforge.org/ scripts.</p>

<p>For this I knocked up a tiny web service wrapper around the gmetric command line. It's very feature light at the moment (I only needed it to collect time based stats at regular intervals) but it could be made more featureful and expose the rest of the gmetric API if needs be. It does it using a very simple URL scheme:</p>

<p><notextile>
&lt;% syntax_colorize :bash, type=:coderay do %>
/{metric-name}/{metric-value}/
&lt;% end %>
</notextile></p>

<p>So for example I can create metrics on the fly simply using an HTTP client or a web browser.</p>

<p><notextile>
&lt;% syntax_colorize :bash, type=:coderay do %>
/GarethsCommuteTime/3600/
/ExternalPageLoad/2.005/
&lt;% end %>
</notextile></p>

<p>The "code is up on GitHub":http://github.com/garethr/gmetric-web and is completely self contained. I've been running it mainly using spawning but any small WSGI server could surfice. I looked very briefly at the API for Ganglia but found the gmetric approach to be much simpler.</p>

<p>And if you're a Ganglia expert and know a much better way of doing this then let me know. Ganglia is awesome, and collecting metrics is both useful and fun (for me at least) but it's not always obvious how to get into creating simple custom metrics which tell you something about your own appliction code.</p>
]]></content>
  </entry>
  
</feed>
